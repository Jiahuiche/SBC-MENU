# ğŸš€ GUÃA COMPLETA: PIPELINE DE CLUSTERING Y CBR PARA MENÃšS

## ğŸ“‹ ÃNDICE
1. [VisiÃ³n General](#visiÃ³n-general)
2. [Estructura de Archivos](#estructura-de-archivos)
3. [Pipeline Paso a Paso](#pipeline-paso-a-paso)
4. [Ventajas del Enfoque](#ventajas-del-enfoque)
5. [PrÃ³ximos Pasos](#prÃ³ximos-pasos)

---

## ğŸ¯ VISIÃ“N GENERAL

Este pipeline transforma la base de datos de Spoonacular en una **estructura optimizada** para:
- âœ… **Clustering**: Agrupar recetas similares
- âœ… **CBR (Case-Based Reasoning)**: Recuperar menÃºs similares a preferencias del usuario
- âœ… **ReducciÃ³n de datos**: De 1602 recetas a ~50-100 representativas

### **Flujo Completo**

```
API Spoonacular
      â†“
[1. ExpandDatabase_Optimized.py]
      â†“
recipes_optimized.json (estructura organizada por features)
      â†“
[2. FeatureEngineering.py]
      â†“
recipe_features_normalized.csv (matriz de features)
      â†“
[3. ClusteringPipeline.py]
      â†“
representative_recipes.json (50-100 recetas representativas)
      â†“
[4. CBR_Engine.py] (futuro)
      â†“
Sistema de recomendaciÃ³n de menÃºs
```

---

## ğŸ“ ESTRUCTURA DE ARCHIVOS

### **Scripts Principales**

| Archivo | PropÃ³sito | Input | Output |
|---------|-----------|-------|--------|
| `ExpandDatabase_Optimized.py` | Obtiene recetas de API | API Spoonacular | `recipes_optimized.json` |
| `FeatureEngineering.py` | Transforma features | `recipes_optimized.json` | 3 CSVs de features |
| `ClusteringPipeline.py` | Clustering y selecciÃ³n | CSVs de features | `representative_recipes.json` |

### **Archivos de Datos**

| Archivo | DescripciÃ³n | TamaÃ±o aprox. |
|---------|-------------|---------------|
| `recipes_optimized.json` | Recetas en estructura optimizada | ~5-10 MB |
| `recipe_features_raw.csv` | Features sin normalizar | ~2-5 MB |
| `recipe_features_normalized.csv` | Features normalizadas (Î¼=0, Ïƒ=1) | ~2-5 MB |
| `recipe_features_pca.csv` | Features con PCA (~50 dims) | ~500 KB |
| `representative_recipes.json` | Recetas representativas finales | ~500 KB |
| `clustering_metrics.json` | MÃ©tricas de calidad del clustering | ~5 KB |

### **Visualizaciones**

| Archivo | DescripciÃ³n |
|---------|-------------|
| `optimal_k_analysis.png` | MÃ©todo del codo + Silhouette |
| `kmeans_clusters.png` | VisualizaciÃ³n 2D de clusters |

---

## ğŸ”„ PIPELINE PASO A PASO

### **PASO 1: Obtener Recetas Optimizadas**

```bash
# Configurar API Key
$env:API_KEY = "tu_api_key_aqui"

# Ejecutar script
python ExpandDatabase_Optimized.py
```

**ParÃ¡metros configurables** (editar en el script):
```python
diets = None          # 'vegan', 'vegetarian', etc.
intolerances = None   # 'dairy', 'gluten', etc.
meal_type = 'main course'  # 'dessert', 'appetizer', 'side dish'
num_recipes = 100
```

**Output**: `recipes_optimized.json`

**Estructura JSON**:
```json
{
  "id": 123,
  "title": "Recipe Name",
  
  "features_numeric": {
    "price_per_serving": 24.5,
    "ready_in_minutes": 35,
    "health_score": 72,
    "calories": 520
  },
  
  "features_categorical": {
    "vegan": false,
    "dish_class": "Main",
    "season": "spring"
  },
  
  "features_text": {
    "cuisines": ["Indian", "Asian"],
    "ingredients": ["chicken", "yogurt", ...]
  },
  
  "taste_profile": {
    "sweetness": 35.2,
    "saltiness": 62.8
  },
  
  "derived_features": {
    "complexity_score": 8.5,
    "value_score": 3.4,
    "price_category": "mid"
  },
  
  "metadata": {
    "wine_pairing": "Aromatic white wine",
    "kosher": false,
    "halal": true
  }
}
```

---

### **PASO 2: Feature Engineering**

```bash
python FeatureEngineering.py
```

**Proceso interno**:
1. âœ… Extrae 16 features numÃ©ricas
2. âœ… Codifica 9 features categÃ³ricas (One-Hot)
3. âœ… Vectoriza listas de texto (cuisines, diets, dish_types, occasions)
4. âœ… Aplica TF-IDF a ingredientes (50 features)
5. âœ… Normaliza con StandardScaler
6. âœ… Aplica PCA (reducciÃ³n a 50 componentes)

**Outputs**:
- `recipe_features_raw.csv` - ~130-150 columnas
- `recipe_features_normalized.csv` - Mismas columnas normalizadas
- `recipe_features_pca.csv` - 50 componentes principales

**Dimensionalidad**:
```
Features numÃ©ricas:        16
Features categÃ³ricas:      ~20 (despuÃ©s de One-Hot)
Cuisines:                  ~15
Diets:                     ~10
Dish types:                ~15
Occasions:                 ~10
Ingredientes (TF-IDF):     50
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                     ~136 features

DespuÃ©s de PCA:            50 features (95% varianza)
```

---

### **PASO 3: Clustering y SelecciÃ³n de Representativos**

```bash
python ClusteringPipeline.py
```

**Proceso interno**:

#### **3.1 BÃºsqueda de K Ã“ptimo**
- Prueba K desde 5 hasta 30
- Calcula Inertia (mÃ©todo del codo)
- Calcula Silhouette Score
- **Selecciona K con mejor Silhouette**

#### **3.2 K-Means Clustering**
```python
K = 15 (ejemplo)
â†’ 15 clusters
â†’ Silhouette: 0.45 (bueno si > 0.4)
â†’ Davies-Bouldin: 0.8 (bueno si < 1.0)
```

#### **3.3 SelecciÃ³n de Representativos**

**MÃ©todo 1: Centroide** (recomendado)
- Calcula centroide de cada cluster
- Selecciona las 3 recetas mÃ¡s cercanas al centroide
- **Ventaja**: Recetas "promedio" del cluster

**MÃ©todo 2: Top Scored**
- Selecciona las 3 recetas con mejor `spoonacular_score`
- **Ventaja**: Recetas de mejor calidad

**Output**:
```
15 clusters Ã— 3 recetas = 45 recetas representativas
```

**Outputs**:
- `representative_recipes.json` - Recetas seleccionadas
- `clustering_metrics.json` - MÃ©tricas de calidad
- `optimal_k_analysis.png` - GrÃ¡ficos de anÃ¡lisis
- `kmeans_clusters.png` - VisualizaciÃ³n 2D

---

## ğŸ“Š MÃ‰TRICAS DE CALIDAD

### **Silhouette Score** (0 a 1)
- **> 0.7**: Clustering excelente
- **0.5-0.7**: Clustering bueno
- **0.4-0.5**: Clustering aceptable
- **< 0.4**: Clustering pobre

### **Davies-Bouldin Index** (0 a âˆ)
- **< 0.5**: Clustering excelente
- **0.5-1.0**: Clustering bueno
- **> 1.0**: Clusters poco separados

### **Calinski-Harabasz Score** (0 a âˆ)
- **Mayor es mejor**
- Indica separaciÃ³n entre clusters

---

## âœ… VENTAJAS DEL ENFOQUE

### **1. Estructura Organizada**
```json
// Antes (ConstruirJSON.py)
{
  "id": 123,
  "vegan": true,
  "price": 24.5,
  "ingredients": [...],
  "cuisines": [...],
  ...  // 41 campos mezclados
}

// DespuÃ©s (ExpandDatabase_Optimized.py)
{
  "id": 123,
  "features_numeric": {...},      // Para clustering
  "features_categorical": {...},  // Para clustering
  "features_text": {...},         // Para vectorizaciÃ³n
  "derived_features": {...},      // Features calculadas
  "metadata": {...}               // Solo para referencia
}
```

### **2. ReducciÃ³n Dimensional Efectiva**
```
1602 recetas originales
  â†“ (Clustering K=15)
15 clusters
  â†“ (3 representativos por cluster)
45 recetas finales (97% reducciÃ³n!)
```

### **3. Features Derivadas Inteligentes**
- `complexity_score`: ingredientes + tiempo
- `value_score`: calidad/precio
- `nutrient_density`: salud/calorÃ­as
- `price_category`: budget/mid/premium

### **4. EliminaciÃ³n de Redundancia**
**Campos eliminados** (no aportan a clustering):
- âŒ `image`, `summary`, `instructions` (presentaciÃ³n)
- âŒ `credits_text`, `source_url` (metadata irrelevante)
- âŒ `low_fodmap`, `sustainable` (baja variabilidad)

**Campos mantenidos** (high-value):
- âœ… NumÃ©ricas: precio, tiempo, scores
- âœ… CategÃ³ricas: restricciones, tipo de plato
- âœ… Texto: ingredientes, cocinas, dietas
- âœ… Sabor: perfil de 5 dimensiones

---

## ğŸ¯ PRÃ“XIMOS PASOS

### **Fase 1: ValidaciÃ³n** âœ… (Completado)
- [x] Estructura optimizada
- [x] Feature engineering
- [x] Clustering pipeline

### **Fase 2: CBR Engine** (Siguiente)
```python
# CBR_Engine.py

def retrieve_similar_cases(user_preferences, representative_recipes):
    """
    Recupera menÃºs similares a preferencias del usuario
    usando distancia en espacio de features
    """
    # 1. Convertir preferencias a vector de features
    user_vector = preferences_to_features(user_preferences)
    
    # 2. Calcular similitud con representativos
    similarities = []
    for recipe in representative_recipes:
        recipe_vector = recipe_to_features(recipe)
        distance = euclidean_distance(user_vector, recipe_vector)
        similarities.append((recipe, distance))
    
    # 3. Ordenar por similitud
    similarities.sort(key=lambda x: x[1])
    
    # 4. Retornar top-k mÃ¡s similares
    return similarities[:k]

def adapt_case(retrieved_recipe, user_constraints):
    """
    Adapta receta recuperada a restricciones especÃ­ficas
    """
    # Ejemplo: Si usuario pide vegan, reemplazar ingredientes
    if user_constraints['vegan'] and not retrieved_recipe['vegan']:
        # Aplicar reglas de adaptaciÃ³n
        adapted = apply_veganization_rules(retrieved_recipe)
        return adapted
    
    return retrieved_recipe
```

### **Fase 3: IntegraciÃ³n con CLIPS**
```clips
;; Cargar representativos en CLIPS
(definstances REPRESENTATIVES::recipes
  ([Rep_Recipe_1] of Recipe ...)
  ([Rep_Recipe_2] of Recipe ...)
  ...
  ([Rep_Recipe_45] of Recipe ...))

;; Regla de matching con CBR
(defrule match-with-cbr
  ?user <- (user-preferences ...)
  =>
  (bind ?similar-cases (python-call retrieve_similar_cases ?user))
  (assert (similar-recipes ?similar-cases)))
```

### **Fase 4: EvaluaciÃ³n**
- PrecisiÃ³n de clustering (validaciÃ³n manual)
- Calidad de recomendaciones CBR
- Tiempo de respuesta del sistema

---

## ğŸ“š REFERENCIAS Y RECURSOS

### **Clustering**
- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [K-Means Tutorial](https://realpython.com/k-means-clustering-python/)

### **CBR (Case-Based Reasoning)**
- [Introduction to CBR](https://en.wikipedia.org/wiki/Case-based_reasoning)
- [CBR Cycle](https://www.researchgate.net/publication/220605751_Case-Based_Reasoning_An_Introduction)

### **Feature Engineering**
- [Feature Engineering Guide](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
- [TF-IDF Explained](https://monkeylearn.com/blog/what-is-tf-idf/)

---

## ğŸ¤ CONTRIBUCIÃ“N

Â¿Mejoras sugeridas?
1. **MÃ¡s features derivadas**: ratios, combinaciones
2. **Clustering hÃ­brido**: combinar K-Means + DBSCAN
3. **ValidaciÃ³n cruzada**: evaluar estabilidad de clusters
4. **UI**: dashboard para visualizar clusters

---

## ğŸ“§ CONTACTO

Para preguntas o sugerencias sobre este pipeline, contactar al equipo de desarrollo.

**Â¡Buena suerte con el clustering y CBR!** ğŸ‰ğŸ½ï¸

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
JUEGOS DE PRUEBA - Sistema CBR de Men√∫s
========================================

Ejecuta autom√°ticamente todos los casos de prueba definidos en test_cases.json
y genera un reporte con los resultados.

Uso:
    python juegos_prueba.py           # Ejecutar todos los tests
    python juegos_prueba.py --verbose # Modo verbose con detalles
    python juegos_prueba.py --test ID # Ejecutar un test espec√≠fico
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional

# A√±adir directorio al path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, SCRIPT_DIR)

# Importar m√≥dulos CBR
from Retrieve import load_case_base, retrieve_cases
from Adapt import adapt_menu, load_all_knowledge_bases
from Revise import revise_menu, print_menu_summary
from Retain import retain_case, get_case_base_stats, calculate_similarity_to_base, calculate_novelty, calculate_trace_score, calculate_usefulness


# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

BASE_DIR = os.path.join(SCRIPT_DIR, '..')
CASE_BASE_PATH = os.path.join(BASE_DIR, 'Base_Casos', 'casos_cbr.json')
TEST_CASES_PATH = os.path.join(BASE_DIR, 'Bases_Conocimientos', 'test_cases.json')
RESULTS_PATH = os.path.join(SCRIPT_DIR, 'test_results.json')


# ============================================================================
# FUNCIONES DE CARGA
# ============================================================================

def load_test_cases(filepath: str = TEST_CASES_PATH) -> List[Dict]:
    """Carga los casos de prueba desde el archivo JSON."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('test_cases', [])
    except FileNotFoundError:
        print(f"‚ùå Archivo de tests no encontrado: {filepath}")
        return []
    except json.JSONDecodeError as e:
        print(f"‚ùå Error parseando JSON: {e}")
        return []


# ============================================================================
# EJECUCI√ìN DE UN TEST
# ============================================================================

def run_single_test(
    test_case: Dict,
    case_base: Dict,
    restricciones_db: Dict,
    contexto_db: Dict,
    ontologia_db: Dict,
    pairing_db: Dict,
    verbose: bool = False,
    save_cases: bool = False
) -> Dict:
    """
    Ejecuta un √∫nico caso de prueba.
    
    Args:
        test_case: Caso de prueba con input y expected
        case_base: Base de casos cargada
        restricciones_db, contexto_db, ontologia_db, pairing_db: Bases de conocimiento
        verbose: Si True, muestra informaci√≥n detallada
        save_cases: Si True, permite guardar casos en la base
        
    Returns:
        Dict con resultados del test
    """
    test_id = test_case.get('id', 'UNKNOWN')
    description = test_case.get('description', 'Sin descripci√≥n')
    user_input = test_case.get('input', {})
    expected = test_case.get('expected', {})
    
    result = {
        'test_id': test_id,
        'description': description,
        'status': 'PENDING',
        'passed': False,
        'execution_time': 0,
        'phases': {},
        'error': None
    }
    
    start_time = time.time()
    
    try:
        if verbose:
            print(f"\n{'='*60}")
            print(f"üß™ {test_id}: {description}")
            print(f"{'='*60}")
            print(f"   Input: {user_input}")
        
        # ================================================================
        # FASE RETRIEVE
        # ================================================================
        retrieve_start = time.time()
        retrieved = retrieve_cases(user_input, case_base)
        retrieve_time = time.time() - retrieve_start
        
        result['phases']['retrieve'] = {
            'success': len(retrieved) > 0,
            'cases_found': len(retrieved),
            'best_score': retrieved[0]['score'] if retrieved else 0,
            'time': retrieve_time
        }
        
        if verbose:
            print(f"\n   üì• RETRIEVE: {len(retrieved)} casos encontrados")
            if retrieved:
                print(f"      Mejor caso: {retrieved[0]['case'].get('id_caso')} (score: {retrieved[0]['score']:.3f})")
        
        if not retrieved:
            result['status'] = 'NO_CASES_FOUND'
            result['passed'] = not expected.get('should_find_case', True)
            return result
        
        best_result = retrieved[0]
        best_case = best_result['case']
        
        # ================================================================
        # FASE ADAPT
        # ================================================================
        adapt_start = time.time()
        adaptation = adapt_menu(
            best_result, user_input,
            restricciones_db, contexto_db, ontologia_db, pairing_db
        )
        adapt_time = time.time() - adapt_start
        
        adapted_menu = adaptation.get('menu', {})
        
        # Contar sustituciones
        total_substitutions = 0
        adaptation_steps = []
        for course_name, course_data in adapted_menu.get('courses', {}).items():
            if isinstance(course_data, dict):
                _adaptation = course_data.get('_adaptation', {})
                subs = _adaptation.get('substitutions', [])
                total_substitutions += len(subs)
                adaptation_steps.extend(subs)
        
        result['phases']['adapt'] = {
            'success': True,
            'substitutions': total_substitutions,
            'time': adapt_time
        }
        
        if verbose:
            print(f"   üîß ADAPT: {total_substitutions} sustituciones realizadas")
            print(f"      Tiempo: {adapt_time:.3f}s")
        
        # ================================================================
        # FASE REVISE (sin interacci√≥n)
        # ================================================================
        revise_start = time.time()
        revision = revise_menu(
            adapted_menu,
            user_input.get('restrictions', []),
            user_input.get('culture', user_input.get('cuisine', '')),
            adaptation_steps=adaptation_steps,
            interactive=False  # ¬°Sin interacci√≥n!
        )
        revise_time = time.time() - revise_start
        
        # Simular feedback del usuario basado en performance
        performance = revision.get('performance', 0.8)
        violations = revision.get('violations', [])
        
        # El feedback simulado depende de las violaciones
        if len(violations) == 0:
            simulated_feedback = 0.9
        elif len(violations) <= 2:
            simulated_feedback = 0.7
        else:
            simulated_feedback = 0.5
        
        revision['user_feedback'] = simulated_feedback
        
        result['phases']['revise'] = {
            'success': len(violations) == 0,
            'performance': performance,
            'violations': len(violations),
            'violation_details': violations[:3],  # M√°ximo 3 para el reporte
            'time': revise_time
        }
        
        if verbose:
            print(f"   ‚úÖ REVISE: Performance {performance:.1%}, {len(violations)} violaciones")
            if violations:
                for v in violations[:3]:
                    print(f"      ‚ö†Ô∏è {v.get('ingredient', 'N/A')}: {v.get('reason', 'N/A')}")
        
        # ================================================================
        # FASE RETAIN (sin guardar por defecto en tests)
        # ================================================================
        retain_start = time.time()
        
        # Para tests, usamos una copia de la base para no modificarla
        if save_cases:
            retention = retain_case(
                adapted_menu, user_input, adaptation_steps, revision,
                case_base=case_base, filepath=CASE_BASE_PATH
            )
        else:
            # Calcular utilidad sin guardar
            # Crear estructura del caso para evaluaci√≥n
            new_case_struct = {
                'problema': {
                    'restricciones_alimentarias': user_input.get('restrictions', []),
                    'cultura_preferible': user_input.get('culture', user_input.get('cuisine', ''))
                },
                'solucion': adapted_menu
            }
            
            # Calcular m√©tricas individuales
            perf = revision.get('performance', 0.5)
            sim = calculate_similarity_to_base(new_case_struct, case_base)
            nov = calculate_novelty(new_case_struct, case_base)
            trc = calculate_trace_score(adaptation_steps)
            
            # Calcular utilidad
            usefulness = calculate_usefulness(perf, sim, nov, trc)
            
            retention = {
                'case_saved': False,
                'usefulness': usefulness,
                'reason': 'Test mode - no save'
            }
        
        retain_time = time.time() - retain_start
        
        result['phases']['retain'] = {
            'success': True,
            'usefulness': retention.get('usefulness', 0),
            'would_save': retention.get('usefulness', 0) >= 0.5,
            'time': retain_time
        }
        
        if verbose:
            print(f"   üíæ RETAIN: Utilidad {retention.get('usefulness', 0):.3f}")
        
        # ================================================================
        # EVALUACI√ìN DEL TEST
        # ================================================================
        result['execution_time'] = time.time() - start_time
        
        # Verificar expectativas
        passed = True
        
        # ¬øSe esperaba encontrar casos?
        if expected.get('should_find_case', True) and len(retrieved) == 0:
            passed = False
        
        # ¬øScore m√≠nimo esperado?
        min_score = expected.get('min_score', 0)
        if best_result['score'] < min_score:
            passed = False
        
        # ¬øSin violaciones requerido?
        if expected.get('no_violations', False) and len(violations) > 0:
            passed = False
        
        result['passed'] = passed
        result['status'] = 'PASSED' if passed else 'FAILED'
        
        if verbose:
            status_icon = "‚úÖ" if passed else "‚ùå"
            print(f"\n   {status_icon} Resultado: {result['status']}")
            print(f"   ‚è±Ô∏è Tiempo total: {result['execution_time']:.3f}s")
        
    except Exception as e:
        result['status'] = 'ERROR'
        result['error'] = str(e)
        result['execution_time'] = time.time() - start_time
        
        if verbose:
            print(f"\n   ‚ùå ERROR: {e}")
            import traceback
            traceback.print_exc()
    
    return result


# ============================================================================
# EJECUCI√ìN DE TODOS LOS TESTS
# ============================================================================

def run_all_tests(
    verbose: bool = False,
    save_cases: bool = False,
    specific_test: Optional[str] = None
) -> Dict:
    """
    Ejecuta todos los casos de prueba.
    
    Args:
        verbose: Modo verbose
        save_cases: Si guardar casos en la base
        specific_test: ID de test espec√≠fico a ejecutar
        
    Returns:
        Dict con resumen de resultados
    """
    print("\n" + "="*70)
    print("üß™ JUEGOS DE PRUEBA - Sistema CBR de Men√∫s")
    print("="*70)
    print(f"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Cargar casos de prueba
    test_cases = load_test_cases()
    if not test_cases:
        print("‚ùå No hay casos de prueba para ejecutar")
        return {'error': 'No test cases found'}
    
    # Filtrar si hay test espec√≠fico
    if specific_test:
        test_cases = [t for t in test_cases if t.get('id') == specific_test]
        if not test_cases:
            print(f"‚ùå Test '{specific_test}' no encontrado")
            return {'error': f'Test {specific_test} not found'}
    
    print(f"üìã Tests a ejecutar: {len(test_cases)}")
    
    # Cargar bases de conocimiento
    print("\n‚è≥ Cargando bases de conocimiento...")
    restricciones_db, contexto_db, ontologia_db, pairing_db = load_all_knowledge_bases()
    
    # Cargar base de casos (load_case_base devuelve lista de casos)
    case_base = load_case_base(CASE_BASE_PATH)
    if not case_base:
        print("‚ùå No se pudo cargar la base de casos")
        return {'error': 'Could not load case base'}
    
    print(f"‚úÖ Base de casos cargada: {len(case_base)} casos")
    
    # Ejecutar tests
    results = {
        'timestamp': datetime.now().isoformat(),
        'total_tests': len(test_cases),
        'passed': 0,
        'failed': 0,
        'errors': 0,
        'total_time': 0,
        'test_results': []
    }
    
    print("\n" + "-"*70)
    print("EJECUTANDO TESTS")
    print("-"*70)
    
    for i, test_case in enumerate(test_cases, 1):
        test_id = test_case.get('id', f'TEST_{i}')
        
        if not verbose:
            print(f"\n[{i}/{len(test_cases)}] {test_id}: {test_case.get('description', '')}...")
        
        test_result = run_single_test(
            test_case, case_base,
            restricciones_db, contexto_db, ontologia_db, pairing_db,
            verbose=verbose,
            save_cases=save_cases
        )
        
        results['test_results'].append(test_result)
        results['total_time'] += test_result.get('execution_time', 0)
        
        # Contar resultados
        if test_result['status'] == 'PASSED':
            results['passed'] += 1
            if not verbose:
                print(f"   ‚úÖ PASSED ({test_result['execution_time']:.2f}s)")
        elif test_result['status'] == 'FAILED':
            results['failed'] += 1
            if not verbose:
                print(f"   ‚ùå FAILED ({test_result['execution_time']:.2f}s)")
        else:
            results['errors'] += 1
            if not verbose:
                print(f"   ‚ö†Ô∏è ERROR: {test_result.get('error', 'Unknown')}")
    
    # Mostrar resumen
    print_summary(results)
    
    # Guardar resultados
    save_results(results)
    
    return results


# ============================================================================
# RESUMEN Y REPORTE
# ============================================================================

def print_summary(results: Dict):
    """Imprime un resumen de los resultados."""
    print("\n" + "="*70)
    print("üìä RESUMEN DE RESULTADOS")
    print("="*70)
    
    total = results['total_tests']
    passed = results['passed']
    failed = results['failed']
    errors = results['errors']
    
    pass_rate = (passed / total * 100) if total > 0 else 0
    
    print(f"\n   Total tests:  {total}")
    print(f"   ‚úÖ Passed:    {passed} ({pass_rate:.1f}%)")
    print(f"   ‚ùå Failed:    {failed}")
    print(f"   ‚ö†Ô∏è Errors:    {errors}")
    print(f"\n   ‚è±Ô∏è Tiempo total: {results['total_time']:.2f}s")
    print(f"   ‚è±Ô∏è Promedio por test: {results['total_time']/total:.2f}s")
    
    # Mostrar tests fallidos
    failed_tests = [t for t in results['test_results'] if t['status'] != 'PASSED']
    if failed_tests:
        print("\n" + "-"*50)
        print("Tests no pasados:")
        for t in failed_tests:
            print(f"   ‚Ä¢ {t['test_id']}: {t['status']}")
            if t.get('error'):
                print(f"     Error: {t['error']}")
            if t['phases'].get('revise', {}).get('violations', 0) > 0:
                print(f"     Violaciones: {t['phases']['revise']['violation_details']}")
    
    print("\n" + "="*70)
    
    # Indicador visual final
    if passed == total:
        print("üéâ ¬°TODOS LOS TESTS PASARON!")
    elif pass_rate >= 80:
        print("üëç La mayor√≠a de tests pasaron")
    elif pass_rate >= 50:
        print("‚ö†Ô∏è Algunos tests fallaron")
    else:
        print("‚ùå La mayor√≠a de tests fallaron")
    
    print("="*70 + "\n")


def save_results(results: Dict, filepath: str = RESULTS_PATH):
    """Guarda los resultados en un archivo JSON."""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"üìÅ Resultados guardados en: {filepath}")
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudieron guardar resultados: {e}")


# ============================================================================
# ENTRY POINT
# ============================================================================

def main():
    """Funci√≥n principal."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Ejecuta juegos de prueba del Sistema CBR'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Modo verbose con detalles de cada test'
    )
    parser.add_argument(
        '--test', '-t',
        type=str,
        help='Ejecutar solo un test espec√≠fico (ej: TEST_001)'
    )
    parser.add_argument(
        '--save',
        action='store_true',
        help='Guardar casos generados en la base (no recomendado para tests)'
    )
    
    args = parser.parse_args()
    
    run_all_tests(
        verbose=args.verbose,
        save_cases=args.save,
        specific_test=args.test
    )


if __name__ == "__main__":
    main()
